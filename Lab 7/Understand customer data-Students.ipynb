{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzHB3pyXO1gq"
   },
   "source": [
    "# Analyze customer data from Austin, TX.\n",
    "\n",
    "In this lab, we will analyze various features of customer energy usage data from the Pecan Street dataset. The dataset we aim to explore is for the month of January 2017 from various customers in Austin, TX.\n",
    "\n",
    "Distinguish which houses have rooftop photovoltaic (PV) panels and which ones do not. For the houses that do not have PV panels, try to predict how much money they would save, given Austin's retail energy rates.\n",
    "\n",
    "Given the temperature profile, can you predict which houses have AC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdHG0TYnO1gs"
   },
   "outputs": [],
   "source": [
    "# Start with customary imports.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing, linear_model\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJCggmsWO1gt"
   },
   "source": [
    "## Understand and parse the dataset\n",
    "\n",
    "There are two csv files that we will utilize in this lab:\n",
    "1. 'dataport-metadata.csv': It contains the details of what data is available from each house in the entire Pecan Street dataset.\n",
    "2. 'July2017.csv': It contains the energy data from houses in Austin for the month of July.\n",
    "\n",
    "### Parse the metadata file.\n",
    "\n",
    "Only consider customers from Austin, TX for which electricity usage data is present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hkx0_HkiO1gt",
    "outputId": "23397088-df48-4c9f-84fc-c34f03fa1ea5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed the metadata file successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the 'metadata' file that contains information about individual houses into a pandas dataframe.\n",
    "dfCityCustomers = pd.read_csv(\"dataport-metadata.csv\", index_col=0)\n",
    "\n",
    "# Only consider the houses that are in Austin and have power consumption data.\n",
    "dfCityCustomers = dfCityCustomers.loc[(dfCityCustomers['city'] == 'Austin') & (dfCityCustomers['use'] == 'yes')]\n",
    "\n",
    "# Restrict attention to useful columns.\n",
    "dfCityCustomers = dfCityCustomers[['date_enrolled', 'date_withdrawn',\n",
    "                                   'building_type', 'total_square_footage', 'first_floor_square_footage',\n",
    "                                   'pv', 'air1', 'air2', 'air3', 'airwindowunit1',\n",
    "                                   'gen', 'use', 'grid']]\n",
    "\n",
    "# Replace binary data with zeros and ones.\n",
    "binaryColumns = ['pv', 'air1', 'air2', 'air3', 'airwindowunit1', 'gen', 'use', 'grid']\n",
    "for bColumn in binaryColumns:\n",
    "    dfCityCustomers[bColumn] = dfCityCustomers[bColumn].map({'yes' : 1}).fillna(0)\n",
    "\n",
    "start_day = datetime.datetime.strptime('2017-07-01', '%Y-%m-%d').date()\n",
    "\n",
    "dfCityCustomers['date_enrolled'] = [datetime.datetime.strptime(x, \"%Y-%m-%d\")\n",
    "                                    for x in dfCityCustomers['date_enrolled']]\n",
    "\n",
    "dfCityCustomers = dfCityCustomers.loc[(dfCityCustomers['date_enrolled'] <= '2017-07-01')]\n",
    "print(\"Parsed the metadata file successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeZJuLWBO1gt"
   },
   "source": [
    "### Parse the energy usage data.\n",
    "\n",
    "Here, we shall clean the data.\n",
    "\n",
    "1. The data downloaded from dataport.cloud has a mislabeled column. Correct that.\n",
    "2. Split the 'localhour' field into two fiels: actual date and an hour of day.\n",
    "3. Only choose data from households that are 'Single-Family Homes'.\n",
    "4. Make sure there is data from 31 days.\n",
    "5. Ensure that the metadata includes the square footage for the entire home and its first floor.\n",
    "\n",
    "Finally, create a pandas dataframe where indices are house id's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "OAUMKL2yO1gt",
    "outputId": "14936616-e7f4-430d-bc47-356a74e84bed",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0e431a09ac40>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the data from Jan 2017 from houses in Texas.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdfData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"July2017.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Cleanup item 1. Alter the column names because there is an error in the downloaded data. The column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 'dataid' and 'localhour' are switched in the .csv file. Correct it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 71 fields in line 2570, saw 77\n"
     ]
    }
   ],
   "source": [
    "# Load the data from Jan 2017 from houses in Texas.\n",
    "dfData = pd.read_csv(\"July2017.csv\")\n",
    "\n",
    "# Cleanup item 1. Alter the column names because there is an error in the downloaded data. The column names\n",
    "# 'dataid' and 'localhour' are switched in the .csv file. Correct it.\n",
    "# Cleanup item 2. Split the hour and the day in the field 'localhour' and convert the date to an\n",
    "# actual datetime object.\n",
    "\n",
    "dfData.columns = [x.lstrip() for x in dfData.columns]\n",
    "dfData = dfData[['dataid', 'localhour', 'use', 'grid', 'gen', 'air1', 'air2', 'air3', 'airwindowunit1']]\n",
    "dfData.columns = ['date', 'dataid', 'use', 'grid', 'gen', 'air1', 'air2', 'air3', 'airwindowunit1']\n",
    "\n",
    "dfData['hour'] = [datetime.datetime.strptime(x[:-3], \"%Y-%m-%d %H:%M:%S\").hour for x in dfData['date']]\n",
    "dfData['date'] = [datetime.datetime.strptime(x[:-3], \"%Y-%m-%d %H:%M:%S\").date() for x in dfData['date']]\n",
    "\n",
    "# Create a dataframe where indices are house id's.\n",
    "dfData_houses = pd.DataFrame(columns=['ac', 'pv', 'area', 'area_floor', 'data'])\n",
    "\n",
    "for house_id in dfData['dataid'].unique():\n",
    "\n",
    "    # Make sure that each house with consumption data is also in metadata.\n",
    "    if house_id in dfCityCustomers.index.values:\n",
    "        dfData_one_house = dfData.loc[dfData['dataid'] == house_id]\n",
    "\n",
    "        # Cleanup item 3, 4, 5.\n",
    "        if ((len(dfData_one_house) >= 24 * 31) &\n",
    "        (np.sum(dfData_one_house['use'].values) != 0) &\n",
    "        (dfCityCustomers.loc[house_id, 'building_type'] == 'Single-Family Home') &\n",
    "        (np.isnan(dfCityCustomers.loc[house_id, 'total_square_footage']) == False) &\n",
    "        (np.isnan(dfCityCustomers.loc[house_id, 'first_floor_square_footage']) == False)):\n",
    "\n",
    "            # Create a pandas dataframe with house id's as indices and has the following items:\n",
    "            # 1. Binary status: \"ac\", \"pv\".\n",
    "            # 2. Total square footrage, and the square footage of the first floor.\n",
    "\n",
    "            is_ac_in_house =  (dfCityCustomers.loc[house_id, 'air1']\n",
    "                               or dfCityCustomers.loc[house_id, 'air2']\n",
    "                               or dfCityCustomers.loc[house_id, 'air3']\n",
    "                               or dfCityCustomers.loc[house_id, 'airwindowunit1']\n",
    "                              )\n",
    "\n",
    "            dfData_houses.loc[house_id] = [is_ac_in_house,\n",
    "                                           dfCityCustomers.loc[house_id, 'pv'],\n",
    "                                           dfCityCustomers.loc[house_id, 'total_square_footage'],\n",
    "                                           dfCityCustomers.loc[house_id, 'first_floor_square_footage'],\n",
    "                                           dfData_one_house\n",
    "                                          ]\n",
    "\n",
    "# Define a function that retrieves the hourly energy profile from the column in \"field\" from a\n",
    "# specific house and day. The day is measured as number of days since the start date of Jan 1, 2017.\n",
    "\n",
    "def energy_day(house_id, day, field):\n",
    "    date_day = start_day + datetime.timedelta(days=day)\n",
    "    dfData_house = dfData_houses.loc[house_id, 'data']\n",
    "    return dfData_house.loc[dfData_house['date'] == date_day].sort_values(by=['hour'])[field].values\n",
    "\n",
    "# Define a function that retrieves the aggregate energy consumed (or produced) as detailed in the column\n",
    "# named \"field\" from a specific house over all days.\n",
    "\n",
    "def total_energy_all_days(house_id, field):\n",
    "    total_energy = 0\n",
    "    for day in range(31):\n",
    "        total_energy += np.sum(energy_day(house_id, day, field))\n",
    "    return total_energy\n",
    "\n",
    "\n",
    "# Cleanup item 4 continued: Delete data from houses where the date and hour do not perfectly\n",
    "# align with the 31 days in July, and hours being from 0 to 23.\n",
    "house_ids_to_delete = []\n",
    "for house_id in dfData_houses.index.values:\n",
    "    for day in range(31):\n",
    "        if len(energy_day(house_id, day, 'grid')) != 24:\n",
    "            house_ids_to_delete.append(house_id)\n",
    "            continue\n",
    "\n",
    "dfData_houses.drop(house_ids_to_delete, inplace=True)\n",
    "\n",
    "print(\"Data loaded and parsed successfully from %d single-family homes.\" % (len(dfData_houses)))\n",
    "\n",
    "del dfData , dfCityCustomers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8KBN3jfO1gt"
   },
   "source": [
    "## Distinguish houses with rooftop solar panels from daily energy usage profile.\n",
    "\n",
    "Take data of energy drawn from the grid for 10 days and do logistic regression.\n",
    "\n",
    "### Q1. Explain Logistic regression. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIppn7ceO1gu"
   },
   "source": [
    "Logic Regression is a classification method that determines its output based on one or more inputs. Its outcome is discrete (0 or 1) and it is put through a logistic function that combines the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KCy1xifO1gu",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of houses with PV panels = %d\" % (len(dfData_houses.loc[dfData_houses['pv'] == 1])) )\n",
    "print(\"Number of houses without PV panels = %d\" % (len(dfData_houses.loc[dfData_houses['pv'] == 0])) )\n",
    "\n",
    "XX = []\n",
    "YY = []\n",
    "\n",
    "days_data = random.sample(range(31), 10)\n",
    "\n",
    "for house_id in dfData_houses.index.values:\n",
    "    XX.append(np.ravel([energy_day(house_id, day, 'grid') for day in days_data]))\n",
    "    YY.append(dfData_houses.loc[house_id, 'pv'])\n",
    "\n",
    "YY = np.reshape(YY, (-1, 1))\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(XX, YY, test_size=.2, shuffle=True)\n",
    "\n",
    "train_X = tf.dtypes.cast(train_X, tf.float32)\n",
    "test_X = tf.dtypes.cast(test_X, tf.float32)\n",
    "train_Y = tf.dtypes.cast(train_Y, tf.float32)\n",
    "test_Y = tf.dtypes.cast(test_Y, tf.float32)\n",
    "\n",
    "del XX, YY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3X2yyrK3O1gu"
   },
   "source": [
    "### Q2. Design the neural network. In the next cell, fill in the missing pieces. (30 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lm7aOWKoO1gu"
   },
   "outputs": [],
   "source": [
    "nDimX = np.shape(train_X)[1]\n",
    "nDimY = np.shape(train_Y)[1]\n",
    "\n",
    "weight = # Enter code here\n",
    "bias = # Enter code here\n",
    "\n",
    "trainable_variables = [weight, bias]\n",
    "\n",
    "@tf.function\n",
    "def neuralNetworkModel(X):\n",
    "    global weight, bias\n",
    "    # enter code here (hint: tf.nn.sigmoid may be useful)\n",
    "\n",
    "loss = # Enter code here (hint: tf.losses.BinaryCrossentropy may be useful)\n",
    "optimizer = # Enter code here (hint: the learning rate may need to be small)\n",
    "\n",
    "# Define number of epochs\n",
    "nEpoch = 1000\n",
    "\n",
    "# Define the training scheme\n",
    "def train(model, x_set, y_set):\n",
    "    for epoch in range(nEpoch):\n",
    "        # Fit the data and compute the gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = model(x_set)\n",
    "            loss = loss_fn(y_true=y_set, y_pred=prediction)\n",
    "\n",
    "            # Print update\n",
    "            lossEpoch = loss.numpy()\n",
    "            print(\"Epoch: %d, Loss: = %1.1f\" % (epoch + 1, lossEpoch))\n",
    "\n",
    "            # Optimize the weights\n",
    "            gradients = tape.gradient(loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print (\"Start neural network training.\")\n",
    "train(neuralNetworkModel, train_X, train_Y)\n",
    "\n",
    "test_prediction = tf.math.round(neuralNetworkModel(test_X))\n",
    "test_accuracy = tf.math.reduce_mean(tf.dtypes.cast(tf.math.equal(test_Y, test_prediction), tf.float32))\n",
    "print(\"Accuracy of logistic regression on test data = %.2f percent.\" % (test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mc08mm1TO1gu"
   },
   "source": [
    "### Q3.  Print the classification report on the test data. The function 'classification_report' from 'sklearn.metrics' might prove useful. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8d9wTWYO1gv"
   },
   "outputs": [],
   "source": [
    "# Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWENL9uYO1gv"
   },
   "source": [
    "### Q4. Based on the classification report you obtain, your classifier is better in which of the following tasks? (20 points)\n",
    "1. If it identifies a house to have a PV panel, then it has a PV panel.\n",
    "2. If there is a PV panel, then it identifies that it has a PV panel.\n",
    "\n",
    "Furthermore, complete the code below to plot the energy drawn from the grid from houses with and without PV panels.\n",
    "\n",
    "The classifier is better at identifying the second observation due to its simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TqF_ZvoO1gv"
   },
   "outputs": [],
   "source": [
    "# Plot energy drawn from grid for houses with PV's.\n",
    "house_id_pv = random.sample(list(dfData_houses.loc[dfData_houses['pv'] == 1].index.values), 5)\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, sharey=True, figsize=(15,5))\n",
    "\n",
    "for tt, house_id in enumerate(house_id_pv):\n",
    "    for day in days_data:\n",
    "        # Enter code here\n",
    "        axs[tt].set_title(\"House \" + str(house_id))\n",
    "fig.suptitle('Houses with PV panels.', fontsize=18)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='Hour', ylabel='Energy in kWh')\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "house_id_not_pv = random.sample(list(dfData_houses.loc[dfData_houses['pv'] == 0].index.values), 5)\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, sharey=True, figsize=(15,5))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for tt, house_id in enumerate(house_id_not_pv):\n",
    "    for day in days_data:\n",
    "    # Enter code here\n",
    "    axs[tt].set_title(house_id)\n",
    "\n",
    "fig.suptitle('Houses without PV panels.', fontsize=18)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='Hour', ylabel='Energy in kWh')\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM_h9b1fO1gv"
   },
   "source": [
    "### Q5. Design a classifier to distinguish between houses with and without PV based on the plots. Print your classification report, and compare it with logistic regression. (10 points, bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jb53SjhO1gw"
   },
   "outputs": [],
   "source": [
    "# Compare the performance of a neural network based classifier with an educated guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PV-i5K5O1gw"
   },
   "source": [
    "## What appliance consumes the most power?\n",
    "\n",
    "Thermal loads are almost always the appliances that consume the most power. The amount of power they draw also typically grows with the size of the house. In the following, we have two tasks:\n",
    "1. What percentage of energy consumption is due to an air conditioner?\n",
    "2. Can you derive a linear relationship between household square footage and power consumption from air conditions in July 2017?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-wrBV-kO1gw"
   },
   "outputs": [],
   "source": [
    "for house_id in dfData_houses.index.values:\n",
    "    dfData_houses.at[house_id, 'ac_usage'] = (total_energy_all_days(house_id, 'air1') +\n",
    "                                              total_energy_all_days(house_id, 'air2') +\n",
    "                                              total_energy_all_days(house_id, 'air3') +\n",
    "                                              total_energy_all_days(house_id, 'airwindowunit1')\n",
    "                                          )\n",
    "    dfData_houses.at[house_id, 'total_usage'] = total_energy_all_days(house_id, 'use')\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "axs[0].scatter(dfData_houses['area'].values,\n",
    "               dfData_houses['ac_usage'].values,\n",
    "               c='r', marker='o', label='AC usage'\n",
    "              )\n",
    "axs[0].set_xlabel('Floor area of house (sq. ft.)')\n",
    "axs[0].set_ylabel('Total AC Usage (kWh).')\n",
    "axs[0].set_title('Power consumption from air conditioners July 2017.', fontsize=14)\n",
    "\n",
    "axs[1].hist(x=np.divide(dfData_houses['ac_usage'], dfData_houses['total_usage']), bins=20, alpha=0.6, color='#0504aa', rwidth=0.85)\n",
    "axs[1].set_xlabel('Fraction of AC usage over total power consumption')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "axs[1].set_title('Histogram of AC usage as a fraction of total power consumption.', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLxMVCY4O1gw"
   },
   "source": [
    "### Q6. Based on the above analysis, will you expect the total power consumption from customers to be more or less in October as compared to that in July? How does the above analysis compare to your analysis in Lab 1 on aggregate load prediction in Texas? (10 points)\n",
    "\n",
    "### Q7. Can you think of a business case for the above histogram? (5 points, bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. I expect the power consumption to be less in October than July. It compares similarly to Lab 1 due to how the lower power needed to cool the houses after the summer is over.\n",
    "\n",
    "7. A business case that could be used for the data is determining power consumption needed throughout a summer. This data can be good for estimating cost and consumption of power that businesses may require."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b103XMIxO1gx"
   },
   "source": [
    "### Compute monthly electric bills for each customer\n",
    "\n",
    "Your electricity bill consists of various charges. These charges depend on the utility company you pay your electricity bill to. In Champaign, IL, your distribution utility company is Ameren. In Austin, a major distribution utility company is Austin Energy. Their bill structure is discussed in the following link:\n",
    "https://austinenergy.com/ae/residential/rates/residential-electric-rates-and-line-items\n",
    "\n",
    "Calculate the monthly bill of each household.\n",
    "\n",
    "1. For each customer, compute the total energy consumed, available in the data field \"use\" over the month. Use the tiered rate structure to compute the total power bill for energy consumption.\n",
    "\n",
    "2. For customers with PV panels, compute the total energy produced by the PV panels. Assume that Ameren Energy pays 9.7 cents/kWh for such production, and subtract the amount for solar power production from the power bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_jtuh-BO1gx",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define a function that computes the electricity bill according to the structure defined by Austin Energy.\n",
    "\n",
    "def electricity_bill(consumption):\n",
    "    customer_charge = 10\n",
    "    power_supply_adjustment = 2.895 * consumption / 100.0\n",
    "    community_benefit_charge = (0.154 + 0.124 + 0.335) * consumption / 100.0\n",
    "    regulatory_charge = 1.342 * consumption / 100.0\n",
    "\n",
    "    tier_rate = [2.801, 5.832, 7.814, 9.314, 10.814]\n",
    "    tier_limits = [0, 500, 1000, 1500, 2500, math.inf]\n",
    "    n_tiers = 5\n",
    "\n",
    "    energy_charge = 0\n",
    "\n",
    "    for tier in range(n_tiers):\n",
    "        consumption_tier = min(max(consumption, tier_limits[tier]), tier_limits[tier + 1]) - tier_limits[tier]\n",
    "        energy_charge += consumption_tier * tier_rate[tier] / 100.0\n",
    "\n",
    "    return float('%.2f'%(1.01 * (energy_charge + customer_charge + power_supply_adjustment + community_benefit_charge\n",
    "                   + regulatory_charge)))\n",
    "\n",
    "for house_id in dfData_houses.index.values:\n",
    "    dfData_houses.at[house_id, 'consumption_bill'] = electricity_bill(dfData_houses.loc[house_id, 'total_usage'])\n",
    "    dfData_houses.at[house_id, 'pv_savings'] = total_energy_all_days(house_id, 'gen') * 9.7 / 100.0\n",
    "    dfData_houses.at[house_id, 'electricity_bill'] = dfData_houses.loc[house_id, 'consumption_bill']\n",
    "    - dfData_houses.loc[house_id, 'pv_savings']\n",
    "\n",
    "\n",
    "print(\"Electricity bill computed for all customers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VpwxB0UO1gx"
   },
   "source": [
    "### What is a good indicator of electricity bill for consumption and savings from PV?\n",
    "\n",
    "Electricity consumption significantly depends on floor area of a house. The dependency is even stronger, if the house is equipped with central AC. A scatter plot of the consumption bill against floor area reveals this dependency.\n",
    "\n",
    "Monetary savings from PV panels depends on how many PV panels there are, which way they face, and how that relates to solar insolation. The number of panels installed largely depends on the roof area. The floor area of one of the floors is a good indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzXFSFkhO1gx"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,4))\n",
    "\n",
    "# Plot of electricity bill for consumption against the square footage of the house.\n",
    "\n",
    "axs[0].scatter(dfData_houses['area'].values,\n",
    "               dfData_houses['consumption_bill'].values,\n",
    "               c='r', marker='o', label='Electricity bill from consumption'\n",
    "           )\n",
    "axs[0].set_xlabel('Floor area of house (sq. ft.)')\n",
    "axs[0].set_ylabel('Energy bill (dollars).')\n",
    "axs[0].set_title('Electricity bill from power consumption over July 2017.', fontsize=14)\n",
    "\n",
    "# Plot monetary savings from PV against the area of the first floor.\n",
    "\n",
    "houses_without_pv = dfData_houses.loc[dfData_houses['pv'] == 0].index.values\n",
    "houses_with_pv = dfData_houses.loc[dfData_houses['pv'] == 1].index.values\n",
    "\n",
    "axs[1].scatter(dfData_houses.loc[houses_with_pv, 'area'].values,\n",
    "               dfData_houses.loc[houses_with_pv, 'pv_savings'].values,\n",
    "               c='g', marker='o', label='Savings from PV')\n",
    "\n",
    "axs[1].set_xlabel('Area of first floor (sq. ft.)')\n",
    "axs[1].set_ylabel('Monthly savings (dollars).')\n",
    "axs[1].set_title('Monthly savings from PV panels over July 2017.', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfbLX0KeO1gx"
   },
   "source": [
    "### Given the square footage of the house and the first floor, compute the electricity bill with and without PV.\n",
    "\n",
    "Perform a linear regression on electricity bill and monthly savings from PV. Use the linear fits to compute the anticipated electricity bill with and without PV. Report your anticipated percentage savings with PV.\n",
    "\n",
    "Use the data:\n",
    "1. Square footage of entire house = 2450 sq. ft.\n",
    "2. Square footage of first floor = 1380 sq. ft.\n",
    "\n",
    "### Q8. Fill in the gaps below. Let's say you have $\\$18,000$ to spend on a PV and get $\\$2,500 $ in solar rebate . How long will it take (in years) for you to recover the initial investment?  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuXq3IKbO1gx",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "area_house = 2450\n",
    "area_first_floor = 1380\n",
    "\n",
    "XX = dfData_houses['area'].values.reshape(-1, 1)\n",
    "YY = dfData_houses['consumption_bill'].values.reshape(-1, 1)\n",
    "\n",
    "model_consumption = linear_model.LinearRegression()\n",
    "model_consumption.fit(XX, YY)\n",
    "\n",
    "print(\"Predicted power bill = $%.2f\" %  model_consumption.predict(np.array([area_house]).reshape(-1, 1)))\n",
    "\n",
    "# Enter code here to compute predicted savings from PV\n",
    "\n",
    "del XX, YY"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
